{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45e3009-b8e0-40c5-88d6-f6cd8ec5faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638fc7c1-9809-4145-97e5-c41114893bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import *\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88d6109-75f9-4c76-b36b-11309e0d0570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 42\n",
    "SEED = random_state\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"./runs/test\")\n",
    "\n",
    "gen = torch.manual_seed(random_state)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826fb55e-7d82-4768-b245-94817b06050e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.train_error = []\n",
    "        self.val_error = []\n",
    "        self.test_error = []\n",
    "        self.best_epoch = 0\n",
    "        self._EPOCH_MAX = 5 * 10000\n",
    "        self._EPOCH_CHECK = 100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288a48f7-2975-46c6-a9f4-fb0358658b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Coder3(BaseNet):\n",
    "\n",
    "    def __init__(self, input_size, hidden_layers, hidden_dim, seed) -> None:\n",
    "        super(Coder3, self).__init__()\n",
    "        \n",
    "        assert len(hidden_layers) == 3\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_layers[0]) \n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.layer1.weight)   \n",
    "\n",
    "        \n",
    "        self.layer2 = nn.Linear(hidden_layers[0], hidden_layers[1], bias=False)  \n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.layer2.weight) \n",
    "\n",
    "        self.layer3 = nn.Linear(hidden_layers[1], hidden_layers[2], bias=False) \n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.layer3.weight)  \n",
    "        \n",
    "        self.layer4 = nn.Linear(hidden_layers[2], hidden_dim, bias=False) \n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.uniform_(self.layer4.weight) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.elu = nn.ELU()\n",
    "    \n",
    "\n",
    "class Encoder3(Coder3):    \n",
    "    def __init__(self, batch_size, *args, **kwargs) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        super(Encoder3, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def noiser(self, z_log_var, z_mean):\n",
    "        N = torch.normal(mean=0., std=1., size=(z_log_var.shape[0], self.hidden_dim))\n",
    "        return torch.exp(z_log_var / 2) * N + z_mean\n",
    "        \n",
    "            \n",
    "    def forward(self, x, y):\n",
    "        inp = torch.concat([x, y], dim=1)\n",
    "        out = self.layer1(inp)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        z_mean = self.layer4(out)\n",
    "        z_log_var = self.layer4(out)\n",
    "        \n",
    "        h = self.noiser(z_log_var, z_mean)\n",
    "        return h, z_log_var, z_mean\n",
    "    \n",
    "\n",
    "class Decoder3(Coder3):    \n",
    "    def __init__(self, batch_size, *args, **kwargs) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        super(Decoder3, self).__init__(*args, **kwargs)        \n",
    "            \n",
    "    def forward(self, x_normal, y):\n",
    "        inp = torch.concat([x_normal, y], dim=1)\n",
    "        out = self.layer1(inp)\n",
    "        out = self.elu(out)\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "        out = self.elu(out)\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        out = self.elu(out)\n",
    "\n",
    "        out = self.layer4(out)        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e882444-d780-48f4-b1cb-4d3c65948ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CVAE(BaseNet, nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, hidden_layers, hidden_dim, y_size, batch_size, seed) -> None:\n",
    "        super(CVAE, self).__init__() \n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = x_size + y_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = Encoder3(self.batch_size, self.input_size, hidden_layers, self.hidden_dim, seed=seed)\n",
    "        self.decoder = Decoder3(self.batch_size, y_size + self.hidden_dim, hidden_layers, x_size, seed=seed)\n",
    "    \n",
    "    def get_loss(self, x, y, z_log_var, z_mean):\n",
    "        loss = torch.sum(torch.square(x-y), axis=-1)\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - torch.square(z_mean) - torch.exp(z_log_var), axis=-1)\n",
    "        return torch.mean(loss + kl_loss)\n",
    "    \n",
    "    def cacl_err(self, data, loss):\n",
    "        with torch.no_grad():\n",
    "            X, y = data.get_transformed_data()\n",
    "\n",
    "            out, z_log_var, z_mean = self(X, y)\n",
    "            #out[predict < 0] = 0\n",
    "            #out[predict > 4] = 4\n",
    "            return loss(self, X, out, z_log_var, z_mean).item()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        h, z_log_var, z_mean = self.encoder(x, y)\n",
    "        out = self.decoder(h, y)\n",
    "        return out, z_log_var, z_mean\n",
    "    \n",
    "    \n",
    "    def fit(self, dataset, train_vall_index, test_index, optimizer, path, conf):\n",
    "        \n",
    "        train_data, val_data, test_data = train_val_test_split(\n",
    "            dataset, train_vall_index, test_index,\n",
    "            vall_ratio=0.22,\n",
    "        )\n",
    "        prev_model = {'loss': 10**6}\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, generator=gen)\n",
    "        \n",
    "        for epoch in tqdm.tqdm(range(self._EPOCH_MAX), desc='EPOCHES'):\n",
    "            \n",
    "            track_loss = []\n",
    "            for X_train, y_train in train_loader:  \n",
    "                predict, z_log_var, z_mean = self(X_train,y_train)\n",
    "                loss = self.get_loss(predict, X_train, z_log_var, z_mean)\n",
    "                track_loss.append(loss.detach())\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "           \n",
    "            train = np.mean(track_loss)\n",
    "            val = self.cacl_err(val_data, CVAE.get_loss)\n",
    "            test = self.cacl_err(test_data, CVAE.get_loss)\n",
    "            self.train_error.append(train)\n",
    "            self.val_error.append(val)\n",
    "            self.test_error.append(test)\n",
    "            \n",
    "            epochs_left = epoch + 1\n",
    "            if self.val_error[-1] < prev_model['loss'] and (epochs_left - self.best_epoch) < self._EPOCH_CHECK:\n",
    "                prev_model = {\n",
    "                    'loss': self.val_error[-1],\n",
    "                    'path': f'{path}/tmp/cvae-{conf}-saved.pt',\n",
    "                    'epoch': epochs_left,\n",
    "                }\n",
    "                self.best_epoch = epochs_left\n",
    "                torch.save(self, prev_model['path'])\n",
    "            elif (epoch - self.best_epoch) >= self._EPOCH_CHECK:\n",
    "                break\n",
    "\n",
    "        model = torch.load(prev_model['path'])\n",
    "        model = model.eval()\n",
    "        model.train_error = copy.deepcopy(self.train_error)\n",
    "        model.val_error = copy.deepcopy(self.val_error)\n",
    "        model.test_error = copy.deepcopy(self.test_error)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e36923-e375-49ae-94b0-624d33c113a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cvae_learning(dataset, transformer, lr, x_size, hidden_layers, hidden_dim, y_size, batch_size, path, i, add_seed):\n",
    "    \n",
    "    rs = ShuffleSplit(n_splits=3, test_size=0.1, random_state=SEED)\n",
    "\n",
    "    \n",
    "    for train_vall_index, test_index in rs.split(dataset):                \n",
    "        seed = SEED + add_seed\n",
    "        \n",
    "        model = CVAE(x_size, hidden_layers, hidden_dim, y_size, batch_size, seed=seed).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        conf = str(hidden_layers)\n",
    "        model = model.fit(dataset, train_vall_index, test_index, optimizer, path, conf)\n",
    "        torch.save(model, f'{path}/models/cvae-{conf}-{i}.pt')\n",
    "        i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79da05ef-5d6d-4e0c-8f86-310fdd6367d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_path = f'./data/OA/x_data.csv'\n",
    "y_path = './data/OA/y_data.csv'\n",
    "\n",
    "transformer = MinMaxTransform()\n",
    "\n",
    "dataset = DataProcessor(\n",
    "    x_path=x_path, \n",
    "    y_path=y_path, \n",
    "    transformer=transformer\n",
    ")\n",
    "\n",
    "transformer.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71bdf7c-3de6-4268-85ce-2e7aac244d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:torch.Size([3744, 911]),   y:torch.Size([3744, 8])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X:{dataset.x_data.shape},   y:{dataset.y_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8996d3db-3d22-465f-b199-acf7f98c14d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "run_params = {\n",
    "    'x_size': 911,                    # the size of X data\n",
    "    'y_size': 8,                      # the size of y data\n",
    "    \n",
    "    'hidden_layers': (256, 126, 32),  # type: typle, neuron numbers in hidden layers, len = 3\n",
    "    'hidden_dim': 16,                 # size of latent space\n",
    "    \n",
    "    'lr': 0.001,                      # learing rate of algorithm\n",
    "    'batch_size': 64,                 # batch size\n",
    "    \n",
    "    'add_seed': 9,                    # random seed for new iteration of algorithm\n",
    "    'path': './',                     # path, there models will be saved\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "num_iters = 0\n",
    "\n",
    "#num_iters = cvae_learning(\n",
    "#    dataset=dataset,\n",
    "#    transformer=transformer, \n",
    "#    i=num_iters,\n",
    "#   **run_params\n",
    "#)\n",
    "#writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b09f10-545e-4463-822d-3c079429ffeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5ad1716-739d-4384-afe2-39c826452499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator3(Coder3):    \n",
    "    def __init__(self, batch_size, *args, **kwargs) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        # self.batch_norm1 = nn.BatchNorm1d(hidden_dim[0]) #affine=False without train params\n",
    "        # self.batch_norm2 = nn.BatchNorm1d(hidden_dim[1])\n",
    "        # self.batch_norm3 = nn.BatchNorm1d(hidden_dim[2])\n",
    "        super(Discriminator3, self).__init__(*args, **kwargs)        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer4(out)        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6620529-b497-4282-9639-4886299b5112",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GAN(BaseNet):\n",
    "    def __init__(self, x_size, hidden_layers, hidden_dim, y_size, batch_size, seed) -> None:\n",
    "        super(GAN, self).__init__() \n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = x_size + y_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self._EPOCH_MAX = 2000\n",
    "        self.generator = CVAE(x_size, hidden_layers, hidden_dim, y_size, batch_size, seed=seed)\n",
    "        self.discriminator = Discriminator3(self.batch_size, x_size, hidden_layers, y_size, seed=seed)\n",
    "    \n",
    "    def __ones(self, rows, cols):\n",
    "        return torch.ones_like(torch.empty(rows, cols))\n",
    "        \n",
    "    \n",
    "    def __generator_loss(self, cross_entropy, fake_output, z_log_var, z_mean):\n",
    "        ones =  torch.ones_like(torch.empty(fake_output.size()))\n",
    "        loss = cross_entropy(ones, fake_output)\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - torch.square(z_mean) - torch.exp(z_log_var), axis=-1)\n",
    "        return torch.mean(loss + kl_loss)\n",
    "\n",
    "\n",
    "    def __discriminator_loss(self, cross_entropy, real_output, fake_output):\n",
    "        ones = torch.ones_like(torch.empty(fake_output.size()))\n",
    "        real_loss = cross_entropy(ones, real_output)\n",
    "        fake_loss = cross_entropy(ones, fake_output)\n",
    "        return real_loss + fake_loss\n",
    "    \n",
    "    \n",
    "    def cacl_err(self, cross_entropy, data):\n",
    "        with torch.no_grad():\n",
    "            X, y = data.get_transformed_data()\n",
    "            # generator predict\n",
    "            gen_x, z_log_var, z_mean = self.generator(X, y)\n",
    "                \n",
    "            # calc error with discriminator\n",
    "            real_output = self.discriminator(X) \n",
    "            fake_output = self.discriminator(gen_x)\n",
    "                \n",
    "            gen_loss = self.__generator_loss(cross_entropy, fake_output, z_log_var, z_mean).item()\n",
    "            disc_loss = self.__discriminator_loss(cross_entropy, real_output, fake_output).item()\n",
    "            return (gen_loss, disc_loss)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        h, z_log_var, z_mean = self.generator(x, y)\n",
    "        out = self.decoder(h, y)\n",
    "        return out, z_log_var, z_mean\n",
    "    \n",
    "    def fit(self, dataset, train_vall_index, test_index, gen_optimizer, disc_optimizer, path, conf):\n",
    "        \n",
    "        train_data, val_data, test_data = train_val_test_split(\n",
    "            dataset, train_vall_index, test_index,\n",
    "            vall_ratio=0.22,\n",
    "        )\n",
    "        prev_model = {'loss': 10**6}\n",
    "        cross_entropy = nn.BCELoss()\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, generator=gen)\n",
    "        \n",
    "        for epoch in tqdm.tqdm(range(self._EPOCH_MAX), desc='EPOCHES'):\n",
    "            \n",
    "            loss_hist = [] # (gen_loss, disc_loss)\n",
    "            for X_train, y_train in train_loader:  \n",
    "                \n",
    "                # train generator\n",
    "                gen_x, z_log_var, z_mean = self.generator(X_train, y_train)\n",
    "                fake_output = self.discriminator(gen_x)\n",
    "                \n",
    "                gen_loss = self.__generator_loss(cross_entropy, fake_output, z_log_var, z_mean)\n",
    "                \n",
    "                gen_optimizer.zero_grad()\n",
    "                gen_loss.backward()\n",
    "                gen_optimizer.step()\n",
    "                \n",
    "                # train discriminator\n",
    "                real_output = self.discriminator(X_train) \n",
    "                \n",
    "                gen_x, z_log_var, z_mean = self.generator(X_train, y_train)\n",
    "                fake_output = self.discriminator(gen_x)\n",
    "                \n",
    "                disc_loss = self.__discriminator_loss(cross_entropy, real_output, fake_output)\n",
    "                \n",
    "                disc_optimizer.zero_grad()\n",
    "                disc_loss.backward()\n",
    "                disc_optimizer.step()\n",
    "                \n",
    "                \n",
    "                # track loss\n",
    "                loss_hist.append(\n",
    "                    (gen_loss.detach(), disc_loss.detach())\n",
    "                )\n",
    "                \n",
    "            # track train, val, test loss            \n",
    "            self.train_error.append(\n",
    "                np.mean(loss_hist, axis=0)\n",
    "            )\n",
    "            self.val_error.append(\n",
    "                self.cacl_err(cross_entropy, val_data)\n",
    "            )\n",
    "            self.test_error.append(\n",
    "                self.cacl_err(cross_entropy, test_data)\n",
    "            )\n",
    "            \n",
    "            #epochs_left = epoch + 1\n",
    "            #if self.val_error[-1][1] < prev_model['loss'][1] and (epochs_left - self.best_epoch) < self._EPOCH_CHECK:\n",
    "            #    prev_model = {\n",
    "            #        'loss': self.val_error[-1],\n",
    "            #        'path': f'{path}/tmp/gan-{conf}-saved.pt',\n",
    "            #        'epoch': epochs_left,\n",
    "            #    }\n",
    "            #    self.best_epoch = epochs_left\n",
    "            #    torch.save(self, prev_model['path'])\n",
    "            #elif (epoch - self.best_epoch) >= self._EPOCH_CHECK:\n",
    "            #    break\n",
    "\n",
    "        #model = torch.load(prev_model['path'])\n",
    "        #model = model.eval()\n",
    "        #model.train_error = copy.deepcopy(self.train_error)\n",
    "        #model.val_error = copy.deepcopy(self.val_error)\n",
    "        #model.test_error = copy.deepcopy(self.test_error)\n",
    "        #return model\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a7c01b-2f9c-4e44-8f2b-aa157ce3a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_learning(dataset, transformer, lr, x_size, hidden_layers, hidden_dim, y_size, batch_size, path, i, add_seed):\n",
    "    \n",
    "    rs = ShuffleSplit(n_splits=3, test_size=0.1, random_state=SEED)\n",
    "    beta1 = 0.5\n",
    "    \n",
    "    for train_vall_index, test_index in rs.split(dataset):                \n",
    "        seed = SEED + add_seed\n",
    "        \n",
    "        model = GAN(x_size, hidden_layers, hidden_dim, y_size, batch_size, seed=seed)\n",
    "        gen_optimizer = torch.optim.Adam(model.generator.parameters(), lr=lr) # betas=(0.5, 0.999)\n",
    "        disc_optimizer = torch.optim.Adam(model.discriminator.parameters(), lr=lr)# betas=(0.5, 0.999)\n",
    "\n",
    "        # Train the model\n",
    "        conf = str(hidden_layers)\n",
    "        model = model.fit(dataset, train_vall_index, test_index, gen_optimizer, disc_optimizer, path, conf)\n",
    "        torch.save(model, f'{path}/models/gan-{conf}-{i}.pt')\n",
    "        i += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b3f20c-d791-4581-85cc-fcaddee62187",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_path = f'./data/OA/x_data.csv'\n",
    "y_path = './data/OA/y_data.csv'\n",
    "\n",
    "transformer = MinMaxTransform()\n",
    "\n",
    "dataset = DataProcessor(\n",
    "    x_path=x_path, \n",
    "    y_path=y_path, \n",
    "    transformer=transformer\n",
    ")\n",
    "\n",
    "transformer.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d134f6-e6fd-4702-8720-85eb2195c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCHES: 100%|█████████████████████████████████████████████████████████████████████████| 2000/2000 [11:18<00:00,  2.95it/s]\n",
      "EPOCHES: 100%|█████████████████████████████████████████████████████████████████████████| 2000/2000 [11:12<00:00,  2.97it/s]\n",
      "EPOCHES:  31%|███████████████████████▏                                                  | 626/2000 [03:35<07:53,  2.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m num_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# torch.autograd.set_detect_anomaly(True)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgan_learning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_params\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n",
      "Cell \u001b[0;32mIn [13], line 15\u001b[0m, in \u001b[0;36mgan_learning\u001b[0;34m(dataset, transformer, lr, x_size, hidden_layers, hidden_dim, y_size, batch_size, path, i, add_seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(hidden_layers)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_vall_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/models/gan-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn [12], line 93\u001b[0m, in \u001b[0;36mGAN.fit\u001b[0;34m(self, dataset, train_vall_index, test_index, gen_optimizer, disc_optimizer, path, conf)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# track train, val, test loss            \u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_error\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     90\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(loss_hist, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_error\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 93\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcacl_err\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_error\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcacl_err(cross_entropy, test_data)\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m#epochs_left = epoch + 1\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m#if self.val_error[-1][1] < prev_model['loss'][1] and (epochs_left - self.best_epoch) < self._EPOCH_CHECK:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m#    prev_model = {\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#model.test_error = copy.deepcopy(self.test_error)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#return model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [12], line 31\u001b[0m, in \u001b[0;36mGAN.cacl_err\u001b[0;34m(self, cross_entropy, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcacl_err\u001b[39m(\u001b[38;5;28mself\u001b[39m, cross_entropy, data):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     32\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget_transformed_data()\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# generator predict\u001b[39;00m\n",
      "File \u001b[0;32m~/Science/salts_water_2022/env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:132\u001b[0m, in \u001b[0;36mno_grad.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_params = {\n",
    "    'x_size': 911,                    # the size of X data\n",
    "    'y_size': 8,                      # the size of y data\n",
    "    \n",
    "    'hidden_layers': (256, 126, 32),  # type: typle, neuron numbers in hidden layers, len = 3\n",
    "    'hidden_dim': 16,                 # size of latent space\n",
    "    \n",
    "    'lr': 0.001,                      # learing rate of algorithm\n",
    "    'batch_size': 64,                 # batch size\n",
    "    \n",
    "    'add_seed': 9,                    # random seed for new iteration of algorithm\n",
    "    'path': './',                     # path, there models will be saved\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "num_iters = 0\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "model = gan_learning(\n",
    "    dataset=dataset,\n",
    "    transformer=transformer, \n",
    "    i=num_iters,\n",
    "   **run_params\n",
    ")\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0445644-5a9e-4874-82da-a496e0db5720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
